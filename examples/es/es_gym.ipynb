{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution strategies on OpenAI gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment and specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<EnvSpec, <SerialVecEnv: CartPole-v1, n: 1>>\n",
       "\tObservation space: Box(4,)\n",
       "\tAction space: Discrete(2)\n",
       "\tControl type: Discrete\n",
       "\tT: 500\n",
       "\tMax episode reward: 475.0\n",
       "\tReward range: (-inf, inf)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lagom.envs import make_gym_env\n",
    "from lagom.envs import make_vec_env\n",
    "from lagom.envs import EnvSpec\n",
    "from lagom.envs.vec_env import SerialVecEnv\n",
    "\n",
    "\n",
    "env = make_vec_env(vec_env_class=SerialVecEnv, \n",
    "                   make_env=make_gym_env, \n",
    "                   env_id='CartPole-v1', \n",
    "                   num_env=1, \n",
    "                   init_seed=0)\n",
    "env_spec = EnvSpec(env)\n",
    "env_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalPolicy\n",
       "\tNetwork: Network(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "  )\n",
       "  (action_head): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lagom.core.networks import BaseNetwork\n",
    "from lagom.core.networks import make_fc\n",
    "from lagom.core.networks import ortho_init\n",
    "\n",
    "from lagom.core.policies import CategoricalPolicy\n",
    "from lagom.core.policies import GaussianPolicy\n",
    "\n",
    "\n",
    "class Network(BaseNetwork):\n",
    "    def make_params(self, config):\n",
    "        self.layers = make_fc(input_dim=env_spec.observation_space.flat_dim, hidden_sizes=[32])\n",
    "        self.last_feature_dim = 32\n",
    "\n",
    "    def init_params(self, config):\n",
    "        for layer in self.layers:\n",
    "            ortho_init(layer, nonlinearity='relu', constant_bias=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "network = Network(config=None, env_spec=env_spec)\n",
    "if env_spec.control_type == 'Discrete':\n",
    "    policy = CategoricalPolicy(config=None, network=network, env_spec=env_spec)\n",
    "elif env_spec.control_type == 'Continuous':\n",
    "    policy = GaussianPolicy(config=None, network=network, env_spec=env_spec)\n",
    "    \n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params = policy.network.num_params\n",
    "num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ES agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lagom.agents import BaseAgent\n",
    "\n",
    "\n",
    "class Agent(BaseAgent):\n",
    "    def __init__(self, policy, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.policy = policy\n",
    "        \n",
    "    def choose_action(self, obs):\n",
    "        obs = torch.from_numpy(np.asarray(obs)).float()  # already batched due to VecEnv\n",
    "        \n",
    "        out_policy = self.policy(obs)\n",
    "        \n",
    "        return out_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lagom.envs import EnvSpec\n",
    "from lagom.runner import TrajectoryRunner\n",
    "\n",
    "\n",
    "def eval_f(parameters, env, N, T):\n",
    "    parameters = torch.from_numpy(parameters).float()\n",
    "    env_spec = EnvSpec(env)\n",
    "    \n",
    "    # Make network and load parameters\n",
    "    network = Network(config=None, env_spec=env_spec)\n",
    "    network.from_vec(parameters)\n",
    "    # Make policy\n",
    "    if env_spec.control_type == 'Discrete':\n",
    "        policy = CategoricalPolicy(config=None, network=network, env_spec=env_spec)\n",
    "    elif env_spec.control_type == 'Continuous':\n",
    "        policy = GaussianPolicy(config=None, network=network, env_spec=env_spec)\n",
    "    # Make agent\n",
    "    agent = Agent(policy=policy, config=None)\n",
    "    \n",
    "    # Create runner\n",
    "    runner = TrajectoryRunner(agent=agent, env=env, gamma=1.0)\n",
    "    \n",
    "    # Take rollouts\n",
    "    D = runner(N=N, T=T)\n",
    "    \n",
    "    # Calculate mean return (no discount)\n",
    "    mean_return = np.mean([sum(trajectory.all_r) for trajectory in D])\n",
    "    \n",
    "    # Negate return to be objective value, because ES does minimization\n",
    "    f = -mean_return\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create master-worker classes for ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lagom.core.es import CMAES\n",
    "from lagom.core.es import OpenAIES\n",
    "\n",
    "from lagom.core.es import BaseESWorker\n",
    "from lagom.core.es import BaseGymESMaster\n",
    "\n",
    "\n",
    "class ESWorker(BaseESWorker):\n",
    "    def f(self, solution, seed):\n",
    "        solution, make_env = solution\n",
    "        \n",
    "        # Create the environment and set seed sent by master\n",
    "        env = make_env(init_seed=seed)\n",
    "        \n",
    "        # Evaluate the solution\n",
    "        function_value = eval_f(parameters=solution, env=env, N=5, T=50)\n",
    "        \n",
    "        return function_value\n",
    "    \n",
    "\n",
    "class ESMaster(BaseGymESMaster):\n",
    "    def make_es(self):\n",
    "        es = CMAES(mu0=[0]*num_params,\n",
    "                   std0=0.5, \n",
    "                   popsize=12)\n",
    "        \n",
    "        return es\n",
    "        \n",
    "    def _process_es_result(self, result):\n",
    "        best_f_val = result['best_f_val']\n",
    "        if self.generation == 0 or (self.generation+1) % 100 == 0:\n",
    "            best_return = -best_f_val  # negate to get back reward\n",
    "            print(f'Best episode reward evalauted at generation {self.generation+1}: {best_return}')\n",
    "            \n",
    "        # Save the parameters in final generation\n",
    "        if (self.generation+1) == self.num_iteration:\n",
    "            np.save('trained_param', result['best_param'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train policy by ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6_w,12)-aCMA-ES (mu_w=3.7,w_1=40%) in dimension 226 (seed=1055468, Fri Sep 14 16:06:52 2018)\n",
      "Best episode reward evalauted at generation 1: 36.0\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "make_env = partial(make_vec_env, \n",
    "                   vec_env_class=SerialVecEnv, \n",
    "                   make_env=make_gym_env, \n",
    "                   env_id='CartPole-v1', \n",
    "                   num_env=1)\n",
    "\n",
    "es = ESMaster(make_env=make_env,\n",
    "              num_iteration=1000, \n",
    "              worker_class=ESWorker, \n",
    "              num_worker=12, \n",
    "              init_seed=0, \n",
    "              daemonic_worker=None)\n",
    "es()\n",
    "\n",
    "print(f'\\nTotal time: {timedelta(seconds=round(time() - t))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.986847748979926"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved parameter\n",
    "parameters = np.load('trained_param.npy')\n",
    "\n",
    "# Make environment\n",
    "env = make_env(seed=None, monitor=True, monitor_dir='logs/')\n",
    "        \n",
    "# Evaluate the solution\n",
    "function_value = rollout(parameters=parameters, \n",
    "                         env=env, \n",
    "                         N=1, \n",
    "                         T=50)\n",
    "function_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
